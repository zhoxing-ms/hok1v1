# HOK1v1 AI 优化指南

## 当前实现方案

### 网络架构
- LSTM核心层 (512单元, 16时间步)
- 实体处理MLP:
  - 英雄: 512->256->128
  - 小兵: 64->64->32
  - 建筑: 64->64->32
- 全局最大池化处理实体特征
- PPO算法:
  - 包含价值、策略和熵损失
  - 梯度裁剪 (0.5)

### 训练配置
- 学习率: 0.0001
- Gamma: 0.995
- Lambda: 0.95
- PPO clip: 0.2
- 熵系数: 0.025

### 奖励结构
| 组件         | 权重   | 说明                 |
|--------------|--------|----------------------|
| 防御塔血量   | 5.0    | 最高优先级           |
| 英雄血量     | 2.0    |                      |
| 死亡         | -1.0   | 惩罚                 |
| 击杀         | -0.6   | 反直觉设置           |
| 补刀         | 0.5    |                      |
| 前进移动     | 0.01   | 数值过低             |

## 优化建议

### 奖励调整
1. 重新平衡击杀/死亡奖励:
   - 击杀改为正奖励 (+1.0)
   - 增加死亡惩罚 (-2.0)
2. 提高前进移动奖励 (0.1)
3. 添加技能使用奖励

### 超参数调优
1. 学习率调度:
   - 初始: 0.0003
   - 衰减至: 0.00003
2. 增加LSTM单元至1024
3. 调整PPO clip范围 (0.1-0.3)
4. 修改熵系数 (0.01-0.05)

### 网络改进
1. 添加注意力机制:
   - 英雄自注意力
   - 实体间交叉注意力
2. 增加MLP维度:
   - 英雄: 1024->512->256
   - 小兵: 128->64->32
3. 添加辅助任务:
   - 下一帧预测
   - 伤害预测

### 训练过程优化
1. 实现课程学习:
   - 从简单场景开始
   - 逐步增加难度
2. 添加与历史版本的自博弈
3. 使用优先经验回放

## 实施计划
1. 首先调整奖励结构
2. 然后调优超参数
3. 最后实现网络改进