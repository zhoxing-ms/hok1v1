代码包介绍
目录介绍
目录名	介绍
ppo	ppo 算法子目录
diy	Do it yourself 用户自定义算法的子目录
conf	配置文件
kaiwu.json	评估时指定对手模型的配置文件
train_test.py	代码正确性测试脚本
其中ppo和diy是Hok1v1支持的 2 个算法，ppo的版本已实现，diy为用户自定义的算法，用户可以自行发挥进行实现。各个算法的子目录结构如下：

目录/文件名	介绍
algorithm/	算法相关，主要是 agent 的实现，包含训练和预测等，详情见算法开发
feature/	样本和回报相关，主要包含用户自定义的数据结构、样本处理方法和回报的计算等等，详情见实现样本处理和回报设计
model/	模型相关，主要是模型的实现，需要实现一个Model类
config.py	该算法下的配置，用户可以任意增加配置或修改配置，注意：SAMPLE_DIM是开悟框架使用配置，不允许删除且应正确配置
train_workflow.py	强化学习的训练流程，详情见强化学习训练流程开发

开发流程
概括来说，我们的开发任务是：开发智能体和智能体的训练流程，这个智能体包含一个可被训练的模型，智能体可以对环境给出的观测进行决策，这个决策作用于环境产生新的观测，此过程通过训练流程控制，不断循环。训练流程还要收集循环过程中产生的每一帧数据，将他们组合成样本数据，智能体可以根据这些样本作为算法的输入，通过算法更新模型。由于Hok1v1采用分布式训练，会启动多个容器，样本需要通过网络通信发送到训练容器（learner）中进行训练，所以需要对样本进行编码方便网络发送，另外智能体需要将learner容器上的模型同步回来。以上任务可以描述为下图：

开发任务描述
开发流程如下：

定义数据结构：一般情况下，环境产生的原始观测数据不能直接作为智能体模型的输入，并且不同的用户开发的智能体一般是不一样的，显然不同的智能体的决策、学习方法的输入输出也是不一样的，所以开发的第一步，我们应该定义智能体模型输入输出的数据结构。包括特征（ObsData）、动作（ActData）、样本（SampleData），其中ObsData和ActData分别作为智能体predict方法的输入和输出，SampleData作为智能体learn方法的输入。
实现样本处理和回报设计：不同的用户设计不同的算法一般需要定义不同的样本数据，例如PPO需要决策动作的概率，但DQN是不需要的，所以还需要用户实现一个将游戏帧转换成SampleData的转换方法sample_process。另外，强化学习智能体的训练需要设计回报，这一部分也是实验重点考察部分，需要由学员自主进行设计，代码包中的示例实现在GameRewardManager中。
算法开发：用户需要实现一个 Agent，Agent需要能够处理用户定义的ObsData和ActData类型，由于不同的用户可能会定义不同的数据结构，同时环境接口输入输出的数据结构是固定的，因此环境接口的输入输出数据和智能体接口的输入输出数据需要进行转换，所以还需要用户实现这些数据结构的转换方法，包括：observation_process和action_process。此外，Agent还需要包含一个模型（一般是神经网络模型），Agent负责与环境交互，产生预测动作并消费样本来训练模型。
实现强化学习训练流程：在实现了 数据结构，数据处理函数，模型和 智能体 以及回报设计等之后，我们还需要实现一个强化学习的训练流程workflow，将所有组件组合起来完成强化学习训练，即智能体通过不断的与环境交互，获取样本数据，更新并迭代模型，直到模型收敛到我们期望的效果。
训练参数配置：在分布式训练时，开悟平台会启动一个样本池，一个模型同步服务，这些组件的相关参数用户可以根据自己的设计进行配置
开发流程
分布式训练架构如下图所示，

diy需要框架训练
特别注意：

分布式训练时，开悟平台会启动一个样本池（样本先进先出），用户的agent.learn(samples)调用将会把样本发送到样本池，训练容器会从样本池中采样样本samples将其传入agent.learn(samples)进行训练，此过程是自动的，用户无需开发额外代码.

Hok1v1虽然提供了两个智能体，但由于红蓝双方的两个智能体是同构的，所以只需训练一个模型，即训练容器（learner）中仅有一个模型实例，所以样本池也只有一个，两个智能体调用agent.learn()将发送样本到同一个样本池；同样地，模型同步服务也只同步一个模型，用户可以按需在恰当时机从模型同步服务加载模型。用户可以为两个智能体加载不同的历史模型，调用agent.load_model(id="latest")将会加载最新模型，若希望加载随机模型则调用agent.load_model(id="random")，若训练过程希望加载某个模型用于评估则可以指定模型id。

由于sample的类型是用户定义的 SampleData，该类型无法直接进行网络传输，需要统一编码成 Numpy.array类型的数据。所以需要用户编写 SampleData2NumpyData函数实现 SampleData类型数据到 Numpy.array类型的转换，同时还要编写 NumpyData2SampleData函数实现 Numpy.array类型数据到 SampleData类型的转换，两个函数作为相对应的编码和解码函数，每一位数据都需要对齐，否则将产生数据错误，无法有效训练。

用户必需要配置的一些参数。根据SampleData转换成的Numpy.array的数据长度设置diy/config.py中配置项SAMPLE_DIM的值

class Config:
    # **注意**，此项必须正确配置，应该与definition.py中的NumpyData2SampleData函数数据对齐，否则可能报样本维度错误
    SAMPLE_DIM = 15584


样本池和模型同步服务的参数

# 样本池容量大小
replay_buffer_capacity = 4096
# 该配置值确定了样本池中有多少样本后开始训练，如preload_ratio=1表示样本池满后开始训练，preload_ratio=2表示样本池中样本达到一半时开始训练
preload_ratio = 1
# 每次训练从样本池中采样的样本数量
train_batch_size = 256
# 每次训练的训练间隔，可以通过这个数值控制样本生成消耗比
learner_train_sleep_seconds = 0.0001
# 模型池大小，如果是采用最新的模型则设置为1; 需要采用历史模型则该值设置为需要的比如50, 模型池也是FIFO模式, 训练产生新模型淘汰旧模型
modelpool_max_save_model_count = 1



定义数据结构
上一节我们介绍过，用户要先定义数据结构（类），包括：包括特征（ObsData）、动作（ActData）、样本（SampleData），这部分的代码，都需要实现在<算法名称>/feature/definition.py中。

我们以 PPO 算法代码包为例，介绍如何定义数据类型：

需要定义相关的数据结构（类）包含观测数据ObsData，动作数据ActData，和样本数据SampleData, 其中ObsData和ActData分别表示智能体模型预测的输入和输出，将会由agent.predict使用；SampleData为样本的数据类型，样本数据将会被agent.learn中的代码进行处理用于模型的训练。这些数据结构（类）包含哪些属性完全由用户自定义，属性名称属性数量没有限制。

create_cls函数用于动态创建一个类，create_cls的第一个参数为类型名称，剩余参数为类的属性，属性默认值为None。以下是代码示例：

# The create_cls function is used to dynamically create a class. The first parameter of the function is the type name,
# and the remaining parameters are the attributes of the class, which should have a default value of None.
# create_cls函数用于动态创建一个类，函数第一个参数为类型名称，剩余参数为类的属性，属性默认值应设为None
ObsData = create_cls("ObsData", 
    feature=None, 
    legal_action=None, 
    lstm_cell=None, 
    lstm_hidden=None
)

ActData = create_cls("ActData",
    action=None,
    d_action=None,
    prob=None,
    value=None,
    lstm_cell=None,
    lstm_hidden=None
)

SampleData = create_cls("SampleData",
    npdata=None
)



注意：必须使用create_cls这个函数创建这些类，若使用普通的类定义方法（class 类名）将无法在开悟平台正确运行。


实现样本处理和回报设计
实现样本处理
用户需要在<算法名称>/feature/definition.py中实现样本处理的代码。

我们依然以 PPO算法代码包为例，介绍如何进行样本处理，需要实现样本处理的工具有：sample_process, FrameCollector。

sample_process：用于将游戏环境中收集的游戏帧集合转换为样本集合，输入为FrameCollector类型，输出为SampleData类型的数据组成的列表

FrameCollector：一方面作为收集器收集游戏的每一帧数据，另一方面，实现了样本处理的绝大部分逻辑

在<算法名称>/feature/definition.py中我们为学员们提供了这部分的示例实现代码，代码较长，用户可以直接参考源码阅读。

注意：sample_process函数头必须包含一个装饰器@attached，否则无法在开悟平台正确运行，代码默认已实现，注意不要删除。

@attached
def sample_process(collector):
    return collector.sample_process()

为了支持分布式训练，样本数据需要进行网络传输，由于SampleData无法直接进行网络传输，需要先转换成Numpy的Array，待传输到对端之后再由np.Array转换成SampleData。所以用户需要实现两个转换函数SampleData2NumpyData和NumpyData2SampleData，这两个函数互为反函数。由于代码包中SampleData的定义比较简单，仅有npdata一个属性，两个函数的实现也比较简单，以下是代码包中这两个函数的示例代码：

注意：这两个函数的实现都必须包含一个装饰器@attached，代码默认已实现，注意不要删除。

@attached
def SampleData2NumpyData(g_data):
    return g_data.npdata


@attached
def NumpyData2SampleData(s_data):
    return SampleData(npdata=s_data)


回报设计
这里的回报特指强化学习中的Reward，用于引导智能体进行策略学习，强化学习优化目标便是最大化累积回报的期望，同时Reward可以作为一个重要指标衡量强化学习训练过程中模型表现的好坏。代码包里提供了一些回报的实现，用户可以参考<算法名称>/feature/reward_manager.py里的GameRewardManager类的设计和实现，代码包里只计算了部分回报，还有一些是没有计算的，留给学员设计和实现，这部分非常开放，建议学员根据对实验和强化学习算法的理解，去实现自己的reward设计。

参考代码包中GameRewardManager对reward的实现，同学们可以通过设计多个奖励子项来帮助智能体获得更好的效果。以下是推荐设计的奖励子项：

reward	类型	描述
hp_point	dense	the rate of health point of hero
tower_hp_point	dense	the rate of health point of tower
money (gold)	dense	the total gold gained
ep_rate	dense	the rate of mana point
death	sparse	being killed
kill	sparse	killing an enemy hero
exp	dense	the experience gained
last_hit	sparse	the last hit for soldier
回报计算方法
默认推荐使用零和reward设计方案，以当前决策帧和上一决策帧的相关数值差作为agent的reward，两个agent的同类reward项相减作为最终reward，最终多种reward项加权求和作为最终的reward返回。回报计算方法不止一种，我们鼓励用户进行创新。
descript

注意：

由于最终的reward是加权求和，各个回报子项的权重是非常重要的。例如，击杀会带来金币和经验的奖励，如果击杀回报权重太高，则可能会导致不推塔只击杀，偏离学习目标。
exp 在英雄满级后置 0，因此可能出现非零和情况

算法开发
Hok1v1的代码包提供以下两种算法：PPO和DIY，后者是我们留下的一个未实现的算法，提供给用户进行自定义算法的实现，以上两个算法的开发流程都是一致的，所以这里以 PPO 的代码为例，讲解 一个用PPO 训练的智能体是如何实现的。

开发模型和智能体
首先，如果我们需要实现一个神经网络模型，我们需要在文件<算法名称>/model/model.py中实现一个Model类，即用pytorch实现一个神经网络模型。

然后，我们需要在文件<算法名称>/algorithm/agent.py中实现一个 Agent类。注意Agent类需要继承 kaiwu_agent.agent.base_agent 的 BaseAgent 类，Agent类的实现需要符合BaseAgent类的接口规范

注意：Agent类必须使用@attached装饰器，代码默认已实现，注意不要删除。

class BaseAgent:
    """
    Agent 的基类，所有的 Agent 都应该继承自这个类"""
    def __init__(self, agent_type="player", device=None, logger=None, monitor=None) -> None:
        raise NotImplementedError

    def learn(self, list_sample_data) -> dict:
        """
        用于学习的函数，接受一个 SampleData 的列表
        """
        raise NotImplementedError

    def predict(self, list_obs_data: list) -> list:
        """
        用于获取动作的函数，接受一个 ObsData 的列表, 返回一个 ActData 的列表
        """
        raise NotImplementedError

    def exploit(self, state_dict: dict) -> list:
        """
        用于获取动作的函数，接受一个 环境提供的 state_dict字典, 返回一个动作，可以被env.step作为参数
        """
        raise NotImplementedError

    def save_model(self, path, id='1'):
        raise NotImplementedError

    def load_model(self, path, id='1'):
        raise NotImplementedError


Agent类有三个核心的方法predict，exploit，和learn，其中predict和exploit方法负责进行预测，区别在于前者是智能体训练时调用的方法，一般是依策略的概率分布采样或引入随机概率，后者是智能体在评估和跑榜时调用的方法，一般是选取策略中概率最高的动作或者策略认为最优的动作；learn方法中实现了核心算法，主要负责消费样本进行模型训练。

注意：

训练和评估时智能体行为有所不同，predict和exploit的接口是完全不同的，用户需要进行区分
agent.predict调用会在另一个进程中进行批量预测以提高算力利用率，此过程是自动的，执行结果与函数调用没有区别，用户无需开发额外代码且无需关心实现的逻辑。
Agent实现的示例代码如下：

"""
PPO/algorithm/agent.py
"""
@attached
class Agent(base_dqn.Agent):
    def __init__(self, agent_type="player", device="cpu", logger=None, monitor=None):
        # 进行若干初始化操作
        # ...

    def _model_inference(self, list_obs_data):
        # 以下是伪代码，详细代码不在此处展示
        list_act_data = [ActData() for _ in self.model()]
        return list_act_data

    @predict_wrapper
    def predict(self, list_obs_data):
        return self._model_inference(list_obs_data, exploit_flag=False)

    @exploit_wrapper
    def exploit(self, state_dict):
        # exploit在提交评估任务时自动调用，参数为env返回的state_dict, 返回env.step使用的action
        obs_data = self.observation_process(state_dict)
        # 模型推理调用_model_inference, 执行本地模型推理
        act_data = self._model_inference([obs_data])[0]
        self.update_status(obs_data, act_data)
        return self.action_process(state_dict, act_data, False)

    @learn_wrapper
    def learn(self, list_sample_data):
        # 算法详细代码不在此处展示


Agent类还有两个方法save_model和load_model分别用来保存模型和加载模型，注意模型默认的前缀model.ckpt-不要改动，可以通过id区分不同的模型，在我们的PPO示例中，模型是标准的pytorch格式，所以保存模型和加载分别调用torch.save和torch.load，以下为示例代码：

"""
PPO/algorithm/agent.py
"""
@attached
class Agent(BaseAgent):
    # ...... 此处省略一些代码

    @save_model_wrapper
    def save_model(self, path=None, id="1"):
        # To save the model, it can consist of multiple files, and it is important to ensure that each filename includes the "model.ckpt-id" field.
        # 保存模型, 可以是多个文件, 需要确保每个文件名里包括了model.ckpt-id字段
        model_file_path = f"{path}/model.ckpt-{str(id)}.pkl"
        torch.save(self.model.state_dict(), model_file_path)
        self.logger.info(f"save model {model_file_path} successfully")

    @load_model_wrapper
    def load_model(self, path=None, id="1"):
        # When loading the model, you can load multiple files, and it is important to ensure that each filename matches the one used during the save_model process.
        # 加载模型, 可以加载多个文件, 注意每个文件名需要和save_model时保持一致
        model_file_path = f"{path}/model.ckpt-{str(id)}.pkl"
        if self.cur_model_name == model_file_path:
            self.logger.info(f"current model is {model_file_path}, so skip load model")
        else:
            self.model.load_state_dict(
                torch.load(
                    model_file_path,
                    map_location=torch.device("cuda:0" if torch.cuda.is_available() else "cpu"),
                )
            )
            self.cur_model_name = model_file_path
            self.logger.info(f"load model {model_file_path} successfully")


注意：以上介绍Agent类的方法必须使用相应的装饰器，例如learn方法必须使用@learn_wrapper装饰器。代码默认已实现，注意不要删除。

最后Agent类还要实现两个方法observation_process和action_process，如上节介绍，目的是将环境接口的输入输出数据和智能体接口的输入输出数据进行转换，示例代码如下：

def action_process(self, state_dict, act_data, is_stochastic):
    frame_no = state_dict["frame_state"]['frameNo']
    if frame_no < START_FRAME:
        act_data.action = FORWARD_ACTION
    if is_stochastic:
        # 采用随机采样动作 action
        return act_data.action
    else:
        # 采用最大概率动作 d_action
        return act_data.d_action
    
def observation_process(self, state_dict):
    feature_vec, legal_action = (
        state_dict["observation"],
        state_dict["legal_action"],
    )
    return ObsData(feature=feature_vec, legal_action=legal_action, lstm_cell=self.lstm_cell, lstm_hidden=self.lstm_hidden)



强化学习训练流程开发
在实现了 数据结构，数据处理函数，回报设计，模型和 智能体 以及其他方法后，我们还需要实现一个强化学习的训练流程（workflow），他将所有组件结合起来完成强化学习训练，即智能体通过不断的与环境交互，获取样本数据，更新并迭代模型，直到模型收敛。

开发workflow
Hok1v1的强化学习训练流程包含：

获取env和agent，实例化游戏帧收集器FrameCollector
监控数据初始化
进入训练主循环
使用用户自定义的配置调用env.reset获得环境的第一帧
调用agent.reset重置智能体，调用frame_collector.reset重置游戏帧收集器FrameCollector
根据训练或评估调用agent.load_model为不同智能体加载不同的模型
进入环境的episode循环
调用agent.train_predict(state_dict)进行训练时的决策
调用agent.observation_process进行特征处理，得到ObsData类型的数据
调用agent.predict, 执行执行智能体模型的决策，得到ActData类型的数据
调用agent.action_process将上一步的ActData类型数据转换为env能处理的动作
计算 reward
收集当前帧的所有信息
调用env.step，执行动作与环境交互, 获取下一帧的状态
若episode结束或达到训练条件，调用sample_process处理当前收集到的所有信息并生成样本
若有样本生成则调用agent.learn进行训练
以适当时间上报适当的监控数据
训练结束，保存最终模型
为了实现这个强化学习训练流程，我们需要在文件<算法名称>/train_workflow.py中实现一个 workflow方法。我们继续以PPO为例

注意：workflow函数需要装饰器@attached，代码默认已实现，注意不要删除。

"""
PPO/train_workflow.py
"""
@attached
def workflow(envs, agents, logger=None, monitor=None):
    # 智能体是否进行训练，与do_predicts相对应
    do_learns = [True, True]
    last_save_model_time = time.time()

    while True:
        for g_data in run_episodes(envs, agents, logger, monitor):
            for index, (d_learn, agent) in enumerate(zip(do_learns, agents)):
                if d_learn and len(g_data[index]) > 0:
                    # learner 采用 while true 训练，此处 learn 实际为发送样本
                    agent.learn(g_data[index])
            g_data.clear()
            

workflow的输入接口为：

envs：环境列表，通过调用开悟场景库， env = kaiwu_env.make("hok1v1") 得到Hok1v1环境, 并作为输入传入 workflow。
agents：智能体列表，通过调用用户实现的 xxx/algorithm/agent.py 实例化 Agent, 并作为输入传入 workflow。
logger：日志，开悟提供的日志组件，接口与常见的 python 的 logging 库一致。
monitor：监控，开悟提供的监控组件，详情参见监控介绍。
下面展示了如何使用 usr_conf 来实现自定义的环境配置：

for episode in range(EPISODES):
    # 用户自定义的环境启动配置
    usr_conf = {
        "diy":{
            "monitor_side": 1,
            "monitor_label": "selfplay",
            "lineups": [[{'hero_id': 133}], [{'hero_id': 508}]],
        }
    }

    # 重置环境, 并获取初始状态
    obs, state_dicts = env.reset(usr_conf=usr_conf)

    # 重置agent
    for i, agent in enumerate(agents):
        player_id = state_dicts[i]["player_id"]
        camp = state_dicts[i]["player_camp"]
        agent.reset(camp, player_id)
        agent.load_model(id="latest")
    
    # 重置游戏帧收集器
    frame_collector.reset(num_agents=agent_num)


下面是一个任务循环简单的伪代码实现，基本是开发者之前实现的各个组件的调用：

# episode循环
while True:
    actions = [NONE_ACTION, ] * agent_num
    for index, (d_predict, agent) in enumerate(zip(do_predicts, agents)):
        if d_predict:
            if not is_eval:
                actions[index] = agent.train_predict(state_dicts[index])
            else:
                actions[index] = agent.eval_predict(state_dicts[index])
            
            # 计算 reward
            reward = agents[i].reward_manager.result(state_dicts[i]["frame_state"])
            state_dicts[i]["reward"] = reward
            # 构造并保存游戏帧
            frame = build_frame(agent, state_dicts[index])
            frame_collector.save_frame(frame, agent_id=index)
            

    # Interact with the environment, execute actions, get the next state
    # 与环境交互, 执行动作, 获取下一步的状态
    frame_no, _, _, terminated, truncated, state_dicts = env.step(actions)

    step += 1

    # 正常结束或超时退出
    if terminated or truncated:
        for index, (d_predict, agent) in enumerate(zip(do_predicts, agents)):
            if d_predict and not is_eval:
                frame_collector.save_last_frame(
                    agent_id=index,
                    reward=state_dicts[index]["reward"]["reward_sum"],
                )
        if len(frame_collector) > 0 and not is_eval:
            list_agents_samples = sample_process(frame_collector)
            yield list_agents_samples
        break


下面是监控功能的一个展示：

monitor_data = {
    "reward": total_reward_dicts[main_agent]["reward_sum"],
    "diy_1": total_reward_dicts[main_agent]["forward"],
}

if monitor:
    monitor.put_data({os.getpid(): monitor_data})
    

workflow的另外两个传入参数是monitor和logger，他们皆已被实例化，用户可以把他作为工具根据需要在自己的代码中使用。

训练中评估
注意在上面workflow的伪代码中，根据is_eval的实际布尔值分别调用了agent.train_predict和agent.eval_predict，这是由于我们的workflow支持在训练过程中边训练边与用户指定的对手模型进行对战评估。用户需要在usr_conf中设置monitor_side作为被评估对象，另一个智能体作为评估的对手，对手智能体应根据用户的意图通过agent.load_model()加载期望的模型（或common_ai），训练时评估的数据将会在监控中展示。

评估时的对手模型可以有两种选择，一种是common_ai，这是内置在环境中一个由规则实现的AI，他的能力的固定的；另一种可以是用户保存到模型管理的模型，用户可以将期望用作对手模型的模型id配置在kaiwu.json中（模型id可以前往平台的模型管理查看），我们最多支持配置3个模型，但在一次训练中，建议同学只选择其中一个模型作为评估的对手。以下是kaiwu.json的配置示例：

{
    "model_pool": [609, 608]
}

下图是开悟平台模型管理的截图，我们可以将模型id写入kaiwu.json用于训练时的评估，注意只有检测成功的模型才是有效的。

评估模型列表
以下是使用对手模型进行训练中评估的示例代码：

"""
PPO/train_workflow.py
"""
def run_episodes(envs, agents, logger, monitor):
    # 以下是伪代码
    # 如果是训练中的评估对局，有以下利用方式
    if is_eval:
        # 1. 使用 common_ai
        opponent_agent = "common_ai"

        # 2. 使用对手模型 609
        opponent_agent = "609"

        # 3. 从 common_ai 和对手模型列表中随机选择
        opponent_agent_list = ["common_ai", "609", "608"]
        opponent_agent = opponent_agent_list[random.randint(0,len(opponent_agent_list))]
        
    # 后续代码将根据 opponent_model 进行 agent 的加载


注意：如果选择使用对手模型，那对手模型只能是当前兼容版本的模型，如果版本存在变更导致不兼容性问题，强行使用会导致加载模型失败的报错。

模型保存
代码包中提供的workflow示例代码会保存模型

"""
PPO/train_workflow.py
"""
@attached
def workflow(envs, agents, logger=None, monitor=None):
    # 以下是伪代码
    if Condition satisfied:
        agents[0].save_model()
        last_save_model_time = time.time()

用户也可以在workflow代码中的任意时机调用agent.save_model保存中间模型。 注意：虽然agent.save_model接受path和id两个参数，但在分布式训练时，workflow中调用该接口传入的参数会被框架覆盖成实际的模型保存路径以及最新的训练步数。

为了避免用户保存模型的频率过于频繁，模型保存会有安全限制，限制规则如下：

保存模型的频率限制: 2次/分钟
单个任务保存模型的次数限制：400次

评估模式
开悟平台支持评估模式，帮助用户在训练后评估模型的能力。评估时用户需要在提交任务界面进行配置，包括选择的对手模型、评估局数。另外，训练模式时，用户一般使用agent.train_predict和agent.eval_predict方法进行决策；而在评估模式时，平台会调用agent.exploit方法进行决策，一般情况下，模型在训练和评估时的决策会因算法不同和用户设计不同，而有不同的行为，这部分由用户定义和实现。

评估时将会分别启动两个智能体容器作为AI服务，这个服务只有一个接口即agent.exploit，他的输入即环境env.step返回的state_dicts，输出即作为环境env.step的输入，评估的workflow会分别调用两个智能体的agent.exploit方法进行对战，最后根据智能体胜负情况进行模型能力的判定。以上过程可以描述为下图：

评估任务描述
使用预训练模型
用户在训练模型的过程中，可能希望加载过去已训练过的模型而不是从零开始训练。此时，可以使用开悟平台的`使用预训练模型功能，用户可以在提交训练前选择模型管理中已有的模型，在训练开始前，智能体会预先加载这个模型。特别要注意的是，如果model的结构进行了变更，旧的模型参数将无法加载到新的模型结构中，此时会产生模型加载的报错。


优化方向与建议
我们提供了若干优化方向和优化建议，供用户参考。以下列出了腾讯开悟往届比赛中的参赛队伍及腾讯内部实践中采用的部分参考方案。

参数优化
训练超参数调整：找到适合训练任务的超参数组合，是持续收敛的必要保证。

conf/configure_app.toml

train_batch_size：调整算法一次更新采样的样本的 batch size 大小，可以调整样本吞吐量

调整batch size的同同时，建议恰当地调整学习率，以减少算法对 batch size 的敏感性。相关研究表明，policy gradient 算法（例如：PPO / PPG）对 batch size 敏感，无法通过简单调整学习率，实现 "batch-size 不变性"。因此，在吞吐量足够的情况下，可以进一步选择具有收敛效果最优的 batch size。

Hilton, Jacob, Karl Cobbe, and John Schulman. "Batch size-invariance for policy optimization." Advances in Neural Information Processing Systems. 2022.

dump_model_freq（模型同步频率）：可以影响样本 on-policy 程度和模型同步开销的 Trade-off。

learner_train_sleep_seconds（算法训练间隔）：learner容器不断从样本池采样数据进行训练，该配置设置两次训练之间的时间间隔，这个参数可以控制样本的生产消耗比，样本生产消耗比对训练效果的影响是很大的。该配置设置较大，则样本生产消耗比变大，设置较小，则样本生产消耗比变小。

<算法名称>/config.py

INIT_LEARNING_RATE_START（初始学习率）：影响每步参数更新幅度（优化器默认采用固定学习率）。
BETA_START（Entropy Loss 权重 
β
β）：Entropy Loss 使输出分布趋于确定性，过大可能导致模型提前收敛，过小则可能影响模型收敛效率，可以结合整体训练阶段或课程学习方案进行调整。
其他 ：config.py中的许多配置对算法效果是有较大影响的，用户可以根据监控分析模型缺陷后进行对应的参数调整。
建模优化
实验需要考察3个英雄，具体英雄私有的状态和行为（如连招之类的复杂行为）还没有进行相关建模，用户可以通过针对性优化提高上限
地图中的“河道之灵”在特征、规则、回报、模型等各方面还没有进行相关建模，用户可以通过针对性优化提高上限
智能体和模型优化
对应位置：<算法名称>/model/model.py

Ye, Deheng, Zhao Liu, Mingfei Sun, Bei Shi, Peilin Zhao, Hao Wu, Hongsheng Yu et al. "Mastering complex control in moba games with deep reinforcement learning." In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 04, pp. 6672-6679. 2020.

descript

主要关注点：

MLP-LSTM 的 Encoder-Decoder 结构
按 Unit 类型切分向量特征，分组编码后拼接为环境整体状态编码
Target Unit 预测基于 Attention 机制实现，其中使用了 Split 的单位特征 Embedding 可以作为 Attention Keys
descript

Ye, Deheng, Guibin Chen, Wen Zhang, Sheng Chen, Bo Yuan, Bo Liu, Jia Chen et al. "Towards playing full moba games with deep reinforcement learning." Advances in Neural Information Processing Systems 33 (2020): 621-632.

主要关注点：

(CNN+MLP) - LSTM 的 Encoder-Decoder 结构
Multi-Head Value，通过分组改进 Value 估计效果，降低方差
使用全局 Perfect Information 辅助 Value 估计
其他可能的优化方向

更合理的解码和预测 —— 时序处理部分，进一步增强建模效果/计算效率；预测部分，实现更完整、更复杂的 Attention 设计

更高的参数效率或 FLOPs 效率 —— 采用更高效的网络结构；优化结构超参数，对效果不敏感的模块减少冗余参数或计算

更强的编解码表征能力 —— 更大规模的 Attention，比如 Transformer Encoder - Decoder 结构，或 Transformer Encoder - RNN Decoder 结构

后置规则（处理模型输出）

具体可以在agent.exploit中实现，在评估时可以生效；在agent.train_predict中实现，在训练时可以生效。

通过开发后置规则，可以支持实现模型输出 -> 动作的二次映射。特定实现下，从 RL Agent 视角看，等价于环境的变化。

奖励机制
具体参考回报设计

可能的优化方向包括：

调整奖励子项权重

静态权重
局内动态权重（e.g. 奖励局内衰减）
新增奖励项目

学习策略
由于实验需要考察3个英雄，如果模型对某个英雄过拟合，则可能在其他两个英雄表现不佳，需要谨慎地考虑其泛化性。

课程学习：将复杂、综合的学习目标分解为多阶段、更小规模的学习任务。

通过预训练模型 + 多阶段训练的方式，分步实现总体学习目标。

例子：通过局间奖励衰减，实现稠密奖励到稀疏奖励的过渡

训练前期，偏重具体行为相关的稠密奖励，引导智能体学会基本操作
训练中期，增强与对局结果强相关的稠密奖励，引导智能体在单局中建立优势
训练后期，调高稀疏奖励权重，引导智能体直接关注最终胜负
参考资料

Narvekar, Sanmit, et al. "Curriculum learning for reinforcement learning domains: A framework and survey." The Journal of Machine Learning Research 21.1 (2020): 7382-7431.

Lilian Weng. "[Blog] Curriculum for Reinforcement Learning. " (2020)

对手模型
可参考训练中评估

可能的优化方向包括：

根据设计，在不同训练阶段设计适当的对手模型，包括common_ai，self-play，具体对手模型等，根据不同的组合和比例引导训练
自定义评估对局，根据评估对局反馈的情况对训练方案进行针对性的优化设计
算法优化
强化学习算法优化：其他条件不变，强化学习算法越高效，收敛至同等能力所需时间越短

Dual-Clip PPO / Value Clip

对 policy loss 进行双重 clip，避免 advantage 取值 outlier 对收敛稳定性的影响。

类似地，可以对 value loss 进行 clip，避免单步更新幅度过大对 value network 收敛的影响。

Value Normalization

Yu, Chao, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. "The surprising effectiveness of ppo in cooperative, multi-agent games." arXiv preprint arXiv:2103.01955 (2021).

descript

对于策略梯度方法，伴随着 RL 过程，value network 学习目标的变化可能比较剧烈，影响了 value 估计的学习效果，进而影响了整体收敛效果与稳定性。实验表明，通过引入 Value 归一化，类 PPO 算法的样本效率可能获得改善。

PPG 算法

Cobbe, Karl W., Jacob Hilton, Oleg Klimov, and John Schulman. "Phasic policy gradient." In International Conference on Machine Learning, pp. 2020-2027. PMLR, 2021.

descript

PPO 算法的一种改进版本，通过改进 value network 训练，提高了样本效率。通过将 value 和 policy 的训练分开交替进行，整合了两种方案：a) 策略与价值支路共享网络； b) 采用独立双网络。传统上，共享网络方案能够帮助共享参数部分学到有用的公共表征，而独立网络方案能够避免不同学习目标之间的干扰—— policy network 应该基于 on-policy 样本进行评估，value network 则完全可以充分利用 off-policy 数据。

SAC 算法

Haarnoja, Tuomas, Aurick Zhou, Pieter Abbeel, and Sergey Levine. "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor." In International conference on machine learning, pp. 1861-1870. PMLR, 2018.

descript

PPO 存在 on-policy 假设，而分布式强化学习场景下，为了实现更高效的并行计算，在 on-policy 程度上存在妥协。SAC 是一种 off-policy 的随机策略 RL 算法，在样本利用方式上和 DDPG 类似，区别在于其生成 stochastic policy，在部分 benchmark 中表现出优于 DDPG 的样本效率。

考虑到更换 PPO 算法涉及较大的开发和调试工作量，推荐大部分队伍优先考虑在 PPO 算法基础上的优化，学有余力的队伍可以直接尝试更换算法。


代码调试
在代码包的根目录，我们提供了代码测试脚本train_test.py，该脚本将使用算法文件夹下train_workflow.py中的workflow进行一次训练，当训练步数>0时判定本次代码测试通过。通过启动一次训练，脚本能够迅速验证流程中的各个环节是否正确进行，确保训练逻辑的准确性。

为避免训练模型时出现因代码问题导致的错误，我们建议用户在正式训练前一定要对代码进行测试。操作如下：

将train_test.py文件中algorithm_name的值修改为需要测试的算法名，算法名需要是algorithm_name_list中的一个。
进入IDE工具栏的【运行与调试】工具，点击下图所示绿色箭头的 运行 按钮。启动后，IDE会开始对代码进行测试，并将运行结果输出到右侧面板下方的终端区域，以方便用户进行观察和分析。
code_test
在代码测试过程中如果遇到错误，则测试流程自动中止。此时用户可以根据下方的终端面板查看错误信息，根据错误信息定位代码的问题。

如果没有遇到错误，则代码测试流程会在一次强化训练结束后自动终止（几分钟左右，请耐心等待），并在下方的终端面板提示Train test succeed。