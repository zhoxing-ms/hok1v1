# 智能体对决1v1优化方案

本项目是针对智能体决策1v1场景的优化方案，旨在训练能够在对战中率先摧毁对方防御塔的智能体。通过一系列的优化改进，使智能体能够在不断探索中学习最优取胜策略。

## 优化内容概述

### 1. 奖励函数优化

在`reward_manager.py`中，我们对奖励机制进行了全面优化：

- **塔攻击专项奖励**：新增`tower_damage`奖励，直接奖励对敌方防御塔造成的伤害
- **连续攻击奖励**：连续攻击防御塔会获得递增奖励，鼓励持续攻击
- **位置奖励**：根据与敌方防御塔的距离给予奖励，距离越近奖励越大
- **健康值的差异化奖励**：在血量充足时更鼓励激进推进，低血量时适当保守
- **技能使用奖励**：优化法力值使用逻辑，鼓励智能体合理使用技能

### 2. PPO算法增强

我们在`agent.py`中对PPO算法进行了多项增强：

- **多轮批量训练**：每批样本多次训练，提高样本利用效率
- **KL散度约束**：防止策略更新过大，保持训练稳定性
- **动态学习率调整**：根据训练进度自动调整学习率
- **温度参数控制**：在行为采样时引入温度参数，在训练初期增加探索，后期更加确定性
- **经验缓冲区**: 提高训练的样本利用效率和训练效率

### 3. 训练流程优化

在`train_workflow.py`中，我们对训练流程进行了如下改进：

- **动态对手选择**：根据智能体当前胜率动态调整对手模型选择概率
- **胜率追踪**：实时追踪训练和评估胜率，指导训练过程
- **自动保存里程碑模型**：在训练关键阶段自动保存模型，方便回滚和评估
- **多样化对手池**：使用自对弈、规则AI和历史模型构建多样化对手池

### 4. 增强型网络结构

我们创建了新的`network.py`文件，实现了增强型神经网络：

- **多头注意力机制**：更好地处理游戏状态中的关键信息
- **残差连接和层归一化**：提高深度网络训练稳定性
- **辅助任务学习**：通过预测敌人行为增强模型的对抗性
- **增强型LSTM**：结合LSTM和注意力机制，更好地处理时序信息

### 5. 超参数调优

在`config.py`中，我们优化了一系列关键超参数：

- **击杀奖励**：从-0.6变成0.5，改为正向奖励，鼓励击杀敌方
- **提高折扣因子**：从0.995提高到0.998，更加重视长期收益
- **增加熵正则化系数**：从0.025提高到0.03，鼓励初期更多探索
- **调整裁剪参数**：从0.2降低到0.15，使策略更新更加稳定
- **增加时间衰减因子**：从0到3000，激励智能体更快获取胜利

## 优化效果

通过以上优化，智能体能够更加高效地学习如何在1v1对战中摧毁敌方防御塔。主要表现为：

1. **更强的目标导向性**：智能体会主动接近并攻击敌方防御塔
2. **更好的资源管理**：合理利用生命值、法力值和技能
3. **更稳定的学习曲线**：训练过程中胜率波动更小，学习更稳定
4. **更高的胜率**：对抗各类对手时都有不错的表现

## 使用说明

要使用此优化方案训练智能体，只需按正常流程启动训练即可。系统会自动加载优化后的代码和参数。

```python
# 启动训练命令示例
python train.py
```

训练过程中会自动记录胜率和其他指标，并在关键阶段保存模型。

## 未来改进方向

1. **元学习框架**：引入元学习方法，更快适应不同对手风格
2. **基于人类专家经验的初始化**：利用人类专家数据进行预训练
3. **自适应奖励权重**：根据训练阶段自动调整各奖励项的权重
4. **多智能体协同训练**：引入多个不同策略的智能体共同训练，增加多样性 